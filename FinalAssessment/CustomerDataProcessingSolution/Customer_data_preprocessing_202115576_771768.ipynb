{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b5f457c",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## <center>DATA PROCESSING TASKS</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae20d5",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db140e-32d6-45f2-be45-963e5ace6b7e",
   "metadata": {},
   "source": [
    "### NOTE: <br>\n",
    "    In an attempt to implement the principle of encapsulation, and in trying to create a reusable application for this assignment, I applied object-oriented programming paradigm in performing the project tasks.\n",
    "    Thus, a class called \"CustomerDataPreProcessing\" would be defined. And, I shall perform each assignment task via its methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a9697",
   "metadata": {},
   "source": [
    "#### Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beaac73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class DataPreProcessing(object):\n",
    "    def __init__(self):\n",
    "        self.intro()\n",
    "        # create folder 'Files' at current working directory for storing useful files\n",
    "        os.makedirs(self.app_folder_loc, exist_ok=True)\n",
    "        self.important_note()\n",
    "\n",
    "    @property\n",
    "    def app_name(self):\n",
    "        return 'DataPreProcessing'\n",
    "\n",
    "    @property\n",
    "    def app_folder_loc(self):\n",
    "        return f'{self.app_name}Output'\n",
    "\n",
    "    @property\n",
    "    def author_name(self):\n",
    "        return 'Osagie Elliot Aibangbee'\n",
    "\n",
    "    @property\n",
    "    def author_st_id(self):\n",
    "        return '202115576'\n",
    "\n",
    "    @property\n",
    "    def degree(self):\n",
    "        return 'MSc Artificial Intelligence and Data Science'\n",
    "\n",
    "    @property\n",
    "    def module(self):\n",
    "        return '771768'\n",
    "\n",
    "    def intro(self):\n",
    "        print(f\"***{self.app_name} App***\\n***November 2021***\\n***By {self.author_name}***\\n***\" +\n",
    "              f\"Student ID: {self.author_st_id}***\\n***Program: {self.degree}***\\n***Module: {self.module}***\")\n",
    "\n",
    "    def important_note(self):\n",
    "        print(f\"\\n\\nNOTE:\\nThis {self.app_name} app will automatically create a folder called {self.app_folder_loc},\" +\n",
    "              \" if it does not already exist, in the current working directory.\")\n",
    "        print(\"This folder shall serve as a container for storing files produced by the app.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def read_from_csv(csv_fname: str) -> list:\n",
    "        \"\"\"\n",
    "        using the csv module, read in data directly from a csv file\n",
    "        and outputs a list of line_lists.\"\"\"\n",
    "\n",
    "        try:\n",
    "            with open(csv_fname, mode='r', encoding='utf8') as csv_f:\n",
    "                return [line for line in csv.reader(csv_f)]\n",
    "        except Exception as err:\n",
    "            print(\"Read-in operation failed!\\nPlease make sure that file exists or check for spelling errors.\")\n",
    "            print(err)\n",
    "\n",
    "    def read_from_json(self, json_fname: str, use_full_path: bool = False) -> list:\n",
    "        \"\"\"\n",
    "        using the json module, read in data directly from a json file\n",
    "        and outputs a list of line_dicts.\"\"\"\n",
    "\n",
    "        if use_full_path:\n",
    "            try:\n",
    "                with open(json_fname, mode='r', encoding='utf8') as json_f:\n",
    "                    return json.load(json_f)\n",
    "            except Exception as err:\n",
    "                print(\n",
    "                    f\"Read-in operation failed!\\nPlease make sure that file exists or check for spelling errors\\n{err}\")\n",
    "        else:\n",
    "            try:\n",
    "                with open(f\"{self.app_folder_loc}//{json_fname}\", mode='r', encoding='utf8') as json_f:\n",
    "                    return json.load(json_f)\n",
    "            except Exception as err:\n",
    "                print(\n",
    "                    f\"Read-in operation failed!\\nPlease make sure that file exists or check for spelling errors\\n{err}\")\n",
    "\n",
    "    def write_json_file_from_dictlist(self, json_fname: str, list_of_dict_obj: list, use_full_path: bool = False):\n",
    "        \"\"\"Write data from a list of dictionaries object in Python to a JSON file.\"\"\"\n",
    "\n",
    "        if use_full_path:\n",
    "            try:\n",
    "                with open(json_fname, mode='w') as json_f:\n",
    "                    json.dump(list_of_dict_obj, json_f, indent=2)\n",
    "                    print(json_fname)\n",
    "            except Exception as err:\n",
    "                print(f\"Sorry, Please check your spellings.\\n{err}\")\n",
    "        else:\n",
    "            try:\n",
    "                with open(f\"{self.app_folder_loc}//{json_fname}\", mode='w') as json_f:\n",
    "                    json.dump(list_of_dict_obj, json_f, indent=2)\n",
    "                    print(json_fname)\n",
    "            except Exception as err:\n",
    "                print(f\"Sorry, Please check your spellings.\\n{err}\")\n",
    "\n",
    "    def create_list_of_dicts(self, csv_fname_or_list: str or list or tuple, headers: list = None,\n",
    "                             verbose: bool = True) -> list:\n",
    "        \"\"\"Create a list of dictionaries either from a csv file or a list of lists\n",
    "        Params: \n",
    "        csv_fname_or_list: must be a str or a list\n",
    "        if csv_fname_or_list is str: it's assumed to be the file name\n",
    "        if csv_fname_or_list is list or tuple: it's assumed to be data\n",
    "        Return:\n",
    "        A list of dictionaries, where each dict is a row of feature,value pairs.\"\"\"\n",
    "        \n",
    "        csv_data, header = None, None\n",
    "        if isinstance(csv_fname_or_list, str):  # read directly from file, if csv file name is given as str\n",
    "            csv_data = tuple(self.read_from_csv(csv_fname_or_list))\n",
    "            header = tuple(self.get_header_from_csv(csv_fname_or_list))\n",
    "        # directly use the given list of lists with no provided headers\n",
    "        elif isinstance(csv_fname_or_list, (tuple, list)) and (headers is None or not len(headers)):\n",
    "            csv_data = tuple(csv_fname_or_list)\n",
    "            header = tuple(csv_data)[0]  # assume first row is headers\n",
    "\n",
    "        elif isinstance(csv_fname_or_list, (tuple, list)) and len(headers):\n",
    "            csv_data = tuple(csv_fname_or_list)  # use provided csv_data list\n",
    "            header = headers  # use provided headers\n",
    "\n",
    "        list_of_dicts = list()\n",
    "        for line_index in range(len(csv_data)):\n",
    "            if line_index == 0:  # ignore the header line\n",
    "                continue\n",
    "            line_dict = dict()  # new dict to keep track of each line header:data pairing\n",
    "            for col_index in range(len(header)):  # going over the current line item one by one\n",
    "                # pair the line item to its corresponding header\n",
    "                line_dict[header[col_index]] = csv_data[line_index][col_index]\n",
    "            list_of_dicts.append(line_dict)\n",
    "        if verbose:\n",
    "            print(f\"There are {len(list_of_dicts)} rows, and {len(header)} columns in the output list of dictionaries\")\n",
    "        return list_of_dicts\n",
    "\n",
    "    @staticmethod\n",
    "    def create_list_of_lists(list_of_dicts: list) -> list:\n",
    "        \"\"\"Output a list of lists using data in a list of dictionaries\n",
    "        Params: \n",
    "        list_of_dicts: must be a list of dictionaries\n",
    "        Return:\n",
    "        A list of lists, where each list is a row of values.\"\"\"\n",
    "\n",
    "        headers = list(list_of_dicts[0].keys())  # create list of column names to serve as headers row\n",
    "        list_of_lists = [headers]  # container for data stored as a list of lists, with headers as the first list\n",
    "        # get remaining rows from list_of_dicts\n",
    "        for row_ind in range(len(list_of_dicts)):  # accessing each dictionary\n",
    "            row_data = []  # container for row values\n",
    "            for ind in range(len(headers)):  # accessing each column name\n",
    "                # add column value according to order of column names in the headers tuple\n",
    "                row_data.append(list_of_dicts[row_ind][headers[ind]])\n",
    "            list_of_lists.append(row_data)\n",
    "        return list_of_lists\n",
    "\n",
    "    @staticmethod\n",
    "    def display_lines_in_list(data_list: list, num_of_lines: int = -1) -> None:\n",
    "        \"\"\"Output to the screen the content of the given list, one after the other.\"\"\"\n",
    "\n",
    "        for ind in range(len(data_list)):\n",
    "            print(f\"{data_list[ind]}\\n\")\n",
    "            if (num_of_lines != -1) and (ind == num_of_lines - 1):\n",
    "                break\n",
    "\n",
    "    def get_header_from_csv(self, csv_fname_or_list: str or list) -> list:\n",
    "        \"\"\"Get the first row in either a csv file or list of lists and Output as headers row.\"\"\"\n",
    "\n",
    "        if isinstance(csv_fname_or_list, str):\n",
    "            return tuple(self.read_from_csv(csv_fname_or_list))[0]\n",
    "        elif isinstance(csv_fname_or_list, (tuple, list)):\n",
    "            return csv_fname_or_list[0]\n",
    "\n",
    "    def classify_from_csv_data(self, csv_fname_or_list: str or list, categorize_as: str, search_keywords: list,\n",
    "                               sub_categories: list = None, first_row_is_headers: bool = True, headers: list = None):\n",
    "        \"\"\"Create mini-dictionary representing a sub_group in each row of a set of data.\n",
    "        Params:\n",
    "        csv_fname_or_list: must be a str or a list of lists.\n",
    "        if csv_fname_or_list is str: it's assumed to be the file name\n",
    "        if csv_fname_or_list is list or tuple: it's assumed to be data.\n",
    "        categorize_as: name of subgroup to serve as key in the mini dictionary.\n",
    "        sub_categories: name of each key in the mini-dictionary.\n",
    "        search_keywords: word(s) to search for in the headers row, and use their\n",
    "        corresponding index to get values in subsequent rows.\n",
    "        first_row_is_headers: indicator for knowing when to use the first row of csv data as headers.\n",
    "        headers: for explicitly providing a list of headers, \n",
    "        usually needed when a headers row isn't in the csv data. \n",
    "        That is, first_row_is_headers is False.\"\"\"\n",
    "\n",
    "        # search kw is not a str, tuple, nor list\n",
    "        if not isinstance(search_keywords, (tuple, list, str)) or not len(search_keywords):\n",
    "            raise ValueError(\"Please provide a search tuple of strings\")\n",
    "        # no header row is given and first row in csv data is not header\n",
    "        if not first_row_is_headers and not len(headers):\n",
    "            raise ValueError(\"Please provide a header row\")\n",
    "\n",
    "        dict_rec = None\n",
    "        # filename given and first row contains column names\n",
    "        # or list of lists is given\n",
    "        if (isinstance(csv_fname_or_list, str) or isinstance(csv_fname_or_list[0], list)) and first_row_is_headers:\n",
    "            dict_rec = self.create_list_of_dicts(csv_fname_or_list)  # directly create dict from file or list of lists\n",
    "        # filename or list of lists given, and list of headers was given\n",
    "        elif ((isinstance(csv_fname_or_list, str) or isinstance(csv_fname_or_list[0], list))\n",
    "              and not first_row_is_headers and len(headers) and isinstance(headers, (tuple, list))):\n",
    "            dict_rec = self.create_list_of_dicts(csv_fname_or_list, headers=headers)\n",
    "        # directly use list of dicts\n",
    "        elif isinstance(csv_fname_or_list[0], dict):\n",
    "            dict_rec = csv_fname_or_list\n",
    "        if dict_rec is None:\n",
    "            raise ValueError(\"Unknown data type found in list of data\")\n",
    "        return [self.categorize_data_in_dict(row, categorize_as, search_keywords, sub_categories)\n",
    "                for row in dict_rec]\n",
    "\n",
    "    @staticmethod\n",
    "    def categorize_data_in_dict(record: dict, categorize_as: str, search_keywords: list,\n",
    "                                sub_categories: list = None):\n",
    "        \"\"\"Takes a single dictionary and creates a new dictionary with a mini dictionary within.\n",
    "        To be used internally by the .classify_from_csv_data() method\"\"\"\n",
    "\n",
    "        if len(sub_categories) != len(search_keywords):\n",
    "            raise Exception(\"Explicitly pass in a list of sub_categories having the same length as columns\")\n",
    "        if sub_categories is None or not len(sub_categories):  # list of sub_categories is not given explicitly\n",
    "            sub_categories = search_keywords  # use default keys on record list\n",
    "        new_rec = dict()\n",
    "        for col_name in list(record.keys()):  # accessing each column on the record dictionary\n",
    "            for col_ind in range(len(search_keywords)):\n",
    "                if search_keywords[col_ind].lower() == col_name.lower():\n",
    "                    new_rec[sub_categories[col_ind]] = record.pop(search_keywords[col_ind])\n",
    "        record[categorize_as] = new_rec\n",
    "        if not len(record):\n",
    "            raise Exception('Empty dict output')\n",
    "        return record\n",
    "\n",
    "    @staticmethod\n",
    "    def get_column_value(dict_line: dict, col_name: str):\n",
    "        \"\"\"Return the value of a feature from a dict row.\"\"\"\n",
    "\n",
    "        for col, val in dict_line.items():\n",
    "            if col.lower() == col_name.lower():\n",
    "                return val\n",
    "\n",
    "    def get_col_values(self, dict_lines: list, col_name: str) -> list:\n",
    "        \"\"\"List the values of a column internally using the get_column_value() method\"\"\"\n",
    "        \n",
    "        return [self.get_column_value(dict_lines[ind], col_name) for ind in range(len(dict_lines))]\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_empty_str(dict_list: list, col_name: str) -> list:\n",
    "        \"\"\"Replace blank str (ie, \"\" or \" \") with None (ie, Python's representation for null values).\n",
    "        Display the index position where they occurred.\n",
    "        And return a new list of dictionaries with blank str values replaced with None.\"\"\"\n",
    "\n",
    "        empty_lines = list()\n",
    "        for ind in range(len(dict_list)):\n",
    "            if dict_list[ind][col_name] in ['', ' ']:\n",
    "                empty_lines.append(ind)\n",
    "                # change empty string to something meaningful\n",
    "                dict_list[ind][col_name] = None\n",
    "\n",
    "        print(f\"Problematic rows for {col_name}: {empty_lines}\")\n",
    "        print(f\"{len(empty_lines)} rows\")\n",
    "        return dict_list\n",
    "\n",
    "    def filter_dictlist(self, dict_list: list, col_name: str, check_value: str,\n",
    "                        use_opposite_cond: bool = False) -> list:\n",
    "        \"\"\"Filter a list of line_dicts based on the values of a given column.\"\"\"\n",
    "\n",
    "        if use_opposite_cond:\n",
    "            filtered_list = [dict_list[ind] for ind in range(len(dict_list)) if\n",
    "                             self.get_column_value(dict_list[ind], col_name) != check_value]\n",
    "            print(f\"Result now has {len(filtered_list)} customers\")\n",
    "            return filtered_list\n",
    "        filtered_list = [dict_list[ind] for ind in range(len(dict_list)) if\n",
    "                         self.get_column_value(dict_list[ind], col_name) == check_value]\n",
    "        print(f\"Result now has {len(filtered_list)} customers\")\n",
    "        return filtered_list\n",
    "\n",
    "    @staticmethod\n",
    "    def change_str_dtype(old_data: str, pref_type: type):\n",
    "        \"\"\"Change the datatype of str to another data type.\n",
    "        Params:\n",
    "        old_data: string value to be converted\n",
    "        pref_type: new or preferred data type for string value.\n",
    "        Note: only types int, float, or bool can be accepted as pref_type.\"\"\"\n",
    "\n",
    "        if not isinstance(old_data, str):  # input is not a string\n",
    "            raise ValueError(\n",
    "                f'Please enter a string for conversion.\\nInstead of {old_data} which is a {type(old_data)}')\n",
    "        if pref_type not in (int, float, bool, 'int', 'float', 'bool'):\n",
    "            raise Exception(f'Enter int, float, or bool as pref_type argument.\\nInstead of {old_data}')\n",
    "\n",
    "        if pref_type in (int, 'int'):\n",
    "            try:\n",
    "                return int(old_data)\n",
    "            except Exception as err:\n",
    "                print(f\"{err}\\nData {old_data} ignored\")\n",
    "        elif pref_type in (float, 'float'):\n",
    "            try:\n",
    "                return float(old_data)\n",
    "            except Exception as err:\n",
    "                print(f\"{err}\\nData {old_data} ignored\")\n",
    "        elif pref_type in (bool, 'bool'):\n",
    "            try:\n",
    "                return eval(old_data)\n",
    "            except Exception as err:\n",
    "                print(f\"{err}\\nData {old_data} ignored\")\n",
    "\n",
    "    def convert_col_dtype(self, dict_list: list, col_name: str, pref_type: type) -> iter:\n",
    "        \"\"\"Use the change_str_dtype() method on a collection of column_names.\"\"\"\n",
    "\n",
    "        dicts_with_new_dtype = [self.change_str_dtype(dict_list[line_idx][col_name], pref_type) for line_idx in\n",
    "                                range(len(dict_list))]\n",
    "        for line_idx in range(len(dict_list)):\n",
    "            dict_list[line_idx][col_name] = dicts_with_new_dtype[line_idx]\n",
    "            yield dict_list[line_idx]\n",
    "\n",
    "    def dtype_converter(self, csv_fname_or_list: str or list or tuple, converter_guide: dict):\n",
    "        \"\"\"Convert csv data from string into their assigned datatypes in converter dictionary\n",
    "        Params:\n",
    "        csv_fname_or_list: name of csv file or data in list\n",
    "        if csv_fname_or_list is str, file name is assumed\n",
    "        if csv_fname_or_list is list or tuple: it's assumed to be data\n",
    "        converter: Here, the target datatype are the keys. While the column_names for conversion are the values.\n",
    "        Using the converter dictionary as a guide, convert the values of each column/key in the dictionary \n",
    "        into a targe datatype, as specified by converter_guide keys.\n",
    "        Return:\n",
    "        data_list: a list of dictionaries.\"\"\"\n",
    "\n",
    "        dict_lines = None\n",
    "        if isinstance(csv_fname_or_list, str):  # assume str is file name\n",
    "            # read in csv file, AKA, original csv data, output is a list of lists\n",
    "            csv_data = self.read_from_csv(csv_fname_or_list)\n",
    "            # convert the original data format to a list of dicts\n",
    "            dict_lines = self.create_list_of_dicts(csv_data, verbose=False)\n",
    "        # assume list is list of dicts\n",
    "        elif isinstance(csv_fname_or_list, (list, tuple)) and isinstance(csv_fname_or_list[0], dict):\n",
    "            dict_lines = csv_fname_or_list\n",
    "        # assume list is list of lists\n",
    "        elif isinstance(csv_fname_or_list, (list, tuple)) and isinstance(csv_fname_or_list[0], list):\n",
    "            # convert from list of lists format to list of dicts\n",
    "            dict_lines = self.create_list_of_dicts(csv_fname_or_list, verbose=False)\n",
    "\n",
    "        data_list, num_col_converted = list(), 0  # count of conversions done on columns\n",
    "        for col_type, col_names in converter_guide.items():\n",
    "            col_count = len(col_names)\n",
    "            if isinstance(col_names, (list, tuple)) and (num_col_converted == 0) and (\n",
    "                    col_count == 1):  # first conversion on a collection of one column\n",
    "                # use original list of dictionaries (dict_lines) as data source\n",
    "                data_list = tuple(self.convert_col_dtype(dict_lines, col_names[0], col_type))\n",
    "                num_col_converted += 1\n",
    "            # subsequent conversions on a collection of one column\n",
    "            elif isinstance(col_names, (list, tuple)) and (num_col_converted != 0) and (col_count == 1):\n",
    "                # continue using previously converted list of dictionaries (data_list) as data source\n",
    "                data_list = tuple(self.convert_col_dtype(data_list, col_names[0], col_type))\n",
    "                num_col_converted += 1\n",
    "            elif isinstance(col_names, str) and (\n",
    "                    num_col_converted == 0):  # first conversion directly on name of column given as str\n",
    "                # use original list of dictionaries (dict_lines) as data source\n",
    "                data_list = tuple(self.convert_col_dtype(dict_lines, col_names, col_type))\n",
    "                num_col_converted += 1\n",
    "            elif isinstance(col_names, str) and (\n",
    "                    num_col_converted != 0):  # subsequent conversions directly on name of column given as str\n",
    "                # continue using previously converted list of dictionaries (data_list) as data source\n",
    "                data_list = tuple(self.convert_col_dtype(data_list, col_names, col_type))\n",
    "                num_col_converted += 1\n",
    "            # when a collection of multiple columns is given for conversion into the same datatype\n",
    "            elif isinstance(col_names, (list, tuple)) and (col_count > 1):\n",
    "                for name in col_names:  # accessing each column in the collection one at a time\n",
    "                    if num_col_converted == 0:  # if this is the first conversion\n",
    "                        # use original list of dictionaries (dict_lines) as data source\n",
    "                        data_list = tuple(self.convert_col_dtype(dict_lines, name, col_type))\n",
    "                        num_col_converted += 1\n",
    "                        continue\n",
    "                    # subsequent conversions on collection of multiple columns to same data type\n",
    "                    # continue using previously converted list of dictionaries (data_list) as data source\n",
    "                    data_list = tuple(self.convert_col_dtype(data_list, name, col_type))\n",
    "                    num_col_converted += 1\n",
    "        return data_list\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date_int_from_str(date_str: str, separator: str = '/') -> tuple:\n",
    "        \"\"\"\n",
    "        Date_str should be in dd/mm/yr or mm/yr format\n",
    "        otherwise explicitly pass the separator character.\"\"\"\n",
    "\n",
    "        split_date = date_str.split(separator)\n",
    "        if len(split_date) == 2:\n",
    "            mm, yr = int(''.join(split_date[0])), int(''.join(split_date[1]))\n",
    "            return mm, yr\n",
    "        elif len(split_date) == 3:\n",
    "            dd, mm, yr = int(''.join(split_date[0])), int(''.join(split_date[1])), int(''.join(split_date[2]))\n",
    "            return dd, mm, yr\n",
    "\n",
    "    def calculate_years_between_two_dates(self, cust_rec: dict, start_date: str, end_date: str) -> float:\n",
    "        \"\"\"Return the number of years between start and end date after considering the\n",
    "        month and year values of both arguments.\n",
    "        Get start and end dates from the customer record dict.\n",
    "        Then pass the output string into the gen_date_int_from_str() method \n",
    "        which generates a tuple of integers for month and year\n",
    "        Params:\n",
    "        cust_rec: dictionary containing one customer's details.\n",
    "        start_date: str of month and year values separated by /\n",
    "        end_date: str of month and year values separated by /\n",
    "        return:\n",
    "        number of years (12 months/year) between both dates.\"\"\"\n",
    "\n",
    "        exp_yr, start_yr = None, None\n",
    "        exp_mon, start_mon = None, None\n",
    "        # continue the search into sub dicts, if not an outer key\n",
    "        if start_date not in cust_rec.keys() or end_date not in cust_rec.keys():\n",
    "            for col_name, value in cust_rec.items():\n",
    "                if isinstance(value, dict):  # check keys of sub dictionaries\n",
    "                    for mini_col, val in value.items():\n",
    "                        if mini_col == start_date:\n",
    "                            start_mon, start_yr = self.get_date_int_from_str(val)\n",
    "                        if mini_col == end_date:\n",
    "                            exp_mon, exp_yr = self.get_date_int_from_str(val)\n",
    "        else:  # both start and end date columns are in the parent dictionary\n",
    "            start_mon, start_yr = self.get_date_int_from_str(cust_rec[start_date])\n",
    "            exp_mon, exp_yr = self.get_date_int_from_str(cust_rec[end_date])\n",
    "        #         print(start_yr, start_mon)\n",
    "        #         print(exp_yr, exp_mon)\n",
    "        # calculate difference between expiry year and start year,\n",
    "        # as well as, expiry month and start month\n",
    "        if any([start_mon, start_yr, exp_mon, exp_yr]) is None:\n",
    "            raise ValueError(\"None value detected!\")\n",
    "        yr_diff, mon_diff = (exp_yr - start_yr), (exp_mon - start_mon)\n",
    "\n",
    "        if mon_diff >= 0:  # if expiry month is greater than or equal to start month\n",
    "            return yr_diff  # year difference is taken\n",
    "        return yr_diff - 1  # else: subtract 1 from year difference\n",
    "\n",
    "    def cc_duration_is_above_decade_detector(self, cust_rec: dict, start_date_col_name: str,\n",
    "                                             end_date_col_name: str) -> bool:\n",
    "        \"\"\"Returns True if years difference between end and start date is ABOVE 10 years.\n",
    "        And False otherwise.\"\"\"\n",
    "        \n",
    "        return self.calculate_years_between_two_dates(cust_rec, start_date_col_name, end_date_col_name) > 10\n",
    "\n",
    "    def column1_per_column2_from_json_file(self, json_fname: str, col1: str, col2: str, new_col: str) -> list:\n",
    "        \"\"\"\n",
    "        Reads data from a json file and \n",
    "        calculates col1/col2 in file record and \n",
    "        outputs a new list of records containing \n",
    "        the same data in json file with an additional \n",
    "        column for the result of our calculation.\"\"\"\n",
    "\n",
    "        json_data = self.read_from_json(json_fname)\n",
    "        # print(json_data[:3])\n",
    "        for ind in range(len(json_data)):\n",
    "            if float(json_data[ind][col2]) > 1:\n",
    "                col1_per_col2 = round(float(json_data[ind][col1]) / float(json_data[ind][col2]), 2)\n",
    "            else:\n",
    "                col1_per_col2 = round(float(json_data[ind][col1]), 2)\n",
    "            json_data[ind][new_col] = col1_per_col2\n",
    "        return sorted(json_data, key=lambda jdata: float(jdata[new_col]), reverse=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def rename_keys(dict_rec: dict, old_names: list, new_names: list):\n",
    "        \"\"\"Change key from old to new.\"\"\"\n",
    "        \n",
    "        if len(old_names) != len(new_names):\n",
    "            raise ValueError(\"Please provide a matching number of old and new names\")\n",
    "        new_rec = dict()\n",
    "        for name, val in dict_rec.items():\n",
    "            if name not in old_names:\n",
    "                new_rec[name] = val\n",
    "                continue\n",
    "            for ind in range(len(old_names)):\n",
    "                if name.lower() == old_names[ind].lower():\n",
    "                    new_rec[new_names[ind]] = val\n",
    "        return new_rec\n",
    "\n",
    "    def rename_col_names_in_json(self, json_fname_or_list: str or list, old_names: list, new_names: list):\n",
    "        \"\"\"Change the names of column from old_names to new_names.\"\"\"\n",
    "        \n",
    "        json_data = None\n",
    "        if isinstance(json_fname_or_list, str):\n",
    "            json_data = self.read_from_json(json_fname_or_list)\n",
    "        elif isinstance(json_fname_or_list, (list, tuple)):\n",
    "            json_data = json_fname_or_list\n",
    "        return [self.rename_keys(json_data[ind], old_names, new_names) for ind in range(len(json_data))]\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_keys(dict_rec: dict, sort_keys_order: list):\n",
    "        \"\"\"Rearrange the order of appearance of each key, value pairing.\"\"\"\n",
    "        \n",
    "        new_rec = dict()\n",
    "        for ind in range(len(sort_keys_order)):\n",
    "            for name, val in dict_rec.items():\n",
    "                if name.lower() == sort_keys_order[ind].lower():\n",
    "                    new_rec[name] = val\n",
    "        for name, val in dict_rec.items():\n",
    "            if name not in list(new_rec.keys()):\n",
    "                new_rec[name] = val\n",
    "        return new_rec\n",
    "\n",
    "    def sort_col_names(self, cust_dict_list: list, sort_keys_order: list):\n",
    "        \"\"\"Rearrange the order of appearance of each row in a list of dictionaries.\"\"\"\n",
    "        \n",
    "        return [self.sort_keys(cust_dict_list[ind], sort_keys_order) for ind in range(len(cust_dict_list))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88630120",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "**Creating an instance/object, 'app', of the class 'CustomerDataPreProcessing'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f585bc-84b3-46e0-b207-6e0b9fd3a1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***DataPreProcessing App***\n",
      "***November 2021***\n",
      "***By Osagie Elliot Aibangbee***\n",
      "***Student ID: 202115576***\n",
      "***Program: MSc Artificial Intelligence and Data Science***\n",
      "***Module: 771768***\n",
      "\n",
      "\n",
      "NOTE:\n",
      "This DataPreProcessing app will automatically create a folder called DataPreProcessingOutput, if it does not already exist, in the current working directory.\n",
      "This folder shall serve as a container for storing files produced by the app.\n"
     ]
    }
   ],
   "source": [
    "app = DataPreProcessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe1093-983a-46f3-bee7-f237b4dca027",
   "metadata": {},
   "source": [
    "___\n",
    "### TASK 1:\n",
    "Read in the provided ACW Data using the CSV library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab38e6-2663-4684-ae81-8d73e1ba0ed0",
   "metadata": {},
   "source": [
    "#### SOLUTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f9d88d1-02a6-4072-a485-90034d6f79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = 'acw_user_data.csv'\n",
    "csv_data = app.read_from_csv(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862fcade-6420-489d-a5ea-6ab8bfef0d8c",
   "metadata": {},
   "source": [
    "**NOTE:**<br> The .read_from_csv() method above directly reads in data from a csv file and returns a list of lists.<br>\n",
    "Each list inside the enclosing list represents one row of data.<br>And each row comprise several column values (23 columns in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5bd1343-de74-4104-92da-3d2601e4e08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1001 records in the acw_user_data.csv file.\n",
      "Note: with column names as first row.\n",
      "\n",
      "First 3 rows are:\n",
      "\n",
      "['Address Street', 'Address City', 'Address Postcode', 'Age (Years)', 'Distance Commuted to Work (miles)', 'Employer Company', 'Credit Card Start Date', 'Credit Card Expiry Date', 'Credit Card Number', 'Credit Card CVV', 'Dependants', 'First Name', 'Bank IBAN', 'Last Name', 'Marital Status', 'Yearly Pension (£)', 'Retired', 'Yearly Salary (£)', 'Sex', 'Vehicle Make', 'Vehicle Model', 'Vehicle Year', 'Vehicle Type']\n",
      "\n",
      "['70 Lydia isle', 'Lake Conor', 'S71 7XZ', '89', '0', 'N/A', '08/18', '11/27', '676373692463', '875', '3', 'Kieran', 'GB62PQKB71416034141571', 'Wilson', 'married or civil partner', '7257', 'True', '72838', 'Male', 'Hyundai', 'Bonneville', '2009', 'Pickup']\n",
      "\n",
      "['00 Wheeler wells', 'Chapmanton', 'L2 7BT', '46', '13.72', 'Begum-Williams', '08/12', '11/26', '4529436854129855', '583', '1', 'Jonathan', 'GB37UMCO54540228728019', 'Thomas', 'married or civil partner', '0', 'False', '54016', 'Male', 'Nissan', 'ATS', '1996', 'Coupe']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 3\n",
    "print(f\"There are {len(csv_data)} records in the {fpath} file.\\nNote: with column names as first row.\\n\\nFirst {count} rows are:\\n\")\n",
    "app.display_lines_in_list(csv_data, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b13d68-d128-4803-941e-72026bde8f47",
   "metadata": {},
   "source": [
    "___\n",
    "### TASK 2: \n",
    "As a CSV file is an entirely flat file structure, we need to convert our data back into its \n",
    "rich structure. <br>\n",
    "Convert all flat structures into nested structures.<br>\n",
    "These are notably: <br>\n",
    "a. Vehicle - consists of make, model, year, and type <br>\n",
    "b. Credit Card - consists of start date, end date, number, security code, and \n",
    "IBAN.<br>\n",
    "c. Address - consists of the main address, city, and postcode.<br>\n",
    "For this task, it may be worthwhile inspecting the CSV headers to see which data \n",
    "columns may correspond to these above. <br>\n",
    "``Note: Ensure that the values read in are appropriately cast to their respective types.``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e2a1f-61d8-4c5e-a075-24412e710643",
   "metadata": {},
   "source": [
    "#### SOLUTION:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41485a7-cf40-4761-abf5-7255bb9f0858",
   "metadata": {},
   "source": [
    "Steps:<br>\n",
    "a) First, change data types from str into more appropriate data types, where necessary.<br>\n",
    "b) Next, I shall convert the data format from flat file structure into nested structures, as instructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3434f720-b75e-434b-9af2-333b5152a2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows, and 23 columns in the output list of dictionaries\n"
     ]
    }
   ],
   "source": [
    "fpath = 'acw_user_data.csv'\n",
    "csv_dict_list = app.create_list_of_dicts(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827688d-ce20-46d7-bd1d-08eff5ca46b7",
   "metadata": {},
   "source": [
    "The ***.create_list_of_dicts() method*** directly reads in data from a csv file using the .read_from_csv() method internally.<br>\n",
    "Changing the structure of each row from a list to a dictionary,<br>\n",
    "stores every row in an enveloping list.<br>\n",
    "And returns the list of dictionary as its output.<br>\n",
    "**Note:**<br>\n",
    "The output list is one row less than the original data in the csv file, because the first row containing the column names has been dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e0d79f-a1bf-46bd-93c7-0e62cc655194",
   "metadata": {},
   "source": [
    "### <br><b>a) Data Type Conversion</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199cfda2",
   "metadata": {},
   "source": [
    "<br>**Note:**<br>\n",
    "Currently, all the values in each column have a str datatype.<br>\n",
    "\n",
    "Now, I shall change to type **float**:<br>\n",
    "***a) 'Distance Commuted to Work (miles)'***<br>\n",
    "    \n",
    "To type **int**:<br>\n",
    "***a) 'Age (Years)'<br>\n",
    "b) 'Yearly Pension (£)'<br>\n",
    "c) 'Yearly Salary (£)'<br>\n",
    "d) 'Credit Card Number'<br>\n",
    "e) 'Credit Card CVV'<br>\n",
    "f) 'Vehicle Year'<br>\n",
    "g) 'Dependants'<br>***\n",
    "\n",
    "To type **bool**:<br>\n",
    "***a) 'Retired'***<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f2e96-9ac3-44de-8473-81c60d25d97b",
   "metadata": {},
   "source": [
    "As defined by the dictionary in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ccb6eb0-015e-44ed-b872-c0d9665dbe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = {'float': 'Distance Commuted to Work (miles)',\n",
    "             'bool': 'Retired',\n",
    "             'int': ('Age (Years)', 'Yearly Pension (£)', 'Yearly Salary (£)', 'Credit Card Number', 'Credit Card CVV', 'Vehicle Year', 'Dependants')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63284aaf-2efb-4eaf-96ed-656ab4c6bb26",
   "metadata": {},
   "source": [
    "**Note:** <br>\n",
    "In the dictionary above, named converter, each key represents a target data type, and the values represent one or more column names to be converted.<br>\n",
    "The \"converter\" serves to guide the process of converting each column into a target datatype, as accurately and efficiently as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "765cb84a-ad46-4015-a323-abc6416a1b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n"
     ]
    }
   ],
   "source": [
    "fpath = 'acw_user_data.csv'\n",
    "csv_dict_list = app.dtype_converter(fpath, converter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114f71a",
   "metadata": {},
   "source": [
    "The above message simply means that any empty str (\"\", or '') encountered during conversion was skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0531c1f-0aac-4465-8e95-944878954603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 records have been converted.\n",
      "\n",
      "First 3 rows are:\n",
      "{'Address Street': '70 Lydia isle', 'Address City': 'Lake Conor', 'Address Postcode': 'S71 7XZ', 'Age (Years)': 89, 'Distance Commuted to Work (miles)': 0.0, 'Employer Company': 'N/A', 'Credit Card Start Date': '08/18', 'Credit Card Expiry Date': '11/27', 'Credit Card Number': 676373692463, 'Credit Card CVV': 875, 'Dependants': 3, 'First Name': 'Kieran', 'Bank IBAN': 'GB62PQKB71416034141571', 'Last Name': 'Wilson', 'Marital Status': 'married or civil partner', 'Yearly Pension (£)': 7257, 'Retired': True, 'Yearly Salary (£)': 72838, 'Sex': 'Male', 'Vehicle Make': 'Hyundai', 'Vehicle Model': 'Bonneville', 'Vehicle Year': 2009, 'Vehicle Type': 'Pickup'}\n",
      "\n",
      "{'Address Street': '00 Wheeler wells', 'Address City': 'Chapmanton', 'Address Postcode': 'L2 7BT', 'Age (Years)': 46, 'Distance Commuted to Work (miles)': 13.72, 'Employer Company': 'Begum-Williams', 'Credit Card Start Date': '08/12', 'Credit Card Expiry Date': '11/26', 'Credit Card Number': 4529436854129855, 'Credit Card CVV': 583, 'Dependants': 1, 'First Name': 'Jonathan', 'Bank IBAN': 'GB37UMCO54540228728019', 'Last Name': 'Thomas', 'Marital Status': 'married or civil partner', 'Yearly Pension (£)': 0, 'Retired': False, 'Yearly Salary (£)': 54016, 'Sex': 'Male', 'Vehicle Make': 'Nissan', 'Vehicle Model': 'ATS', 'Vehicle Year': 1996, 'Vehicle Type': 'Coupe'}\n",
      "\n",
      "{'Address Street': 'Studio 33K Joel walk', 'Address City': 'Randallborough', 'Address Postcode': 'ME3N 1GH', 'Age (Years)': 22, 'Distance Commuted to Work (miles)': 16.02, 'Employer Company': 'Hill-Wright', 'Credit Card Start Date': '11/19', 'Credit Card Expiry Date': '07/27', 'Credit Card Number': 4091726363083888495, 'Credit Card CVV': 422, 'Dependants': 1, 'First Name': 'Antony', 'Bank IBAN': 'GB40CVUE84011545859591', 'Last Name': 'Jones', 'Marital Status': 'married or civil partner', 'Yearly Pension (£)': 0, 'Retired': False, 'Yearly Salary (£)': 68049, 'Sex': 'Male', 'Vehicle Make': 'GMC', 'Vehicle Model': 'Achieva', 'Vehicle Year': 2015, 'Vehicle Type': 'Convertible, Coupe'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 3\n",
    "print(f\"{len(csv_dict_list)} records have been converted.\\n\\nFirst {count} rows are:\")\n",
    "app.display_lines_in_list(csv_dict_list, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db093f2-f6b4-4a14-b981-c76156467be8",
   "metadata": {},
   "source": [
    "***DATA TYPE CONVERSION SUCCESSFUL!!!***<br>\n",
    "Apart from the 19 errors due to the presence of blank strings \"\" in the Dependants column, ***which were all skipped,*** <br>\n",
    "all other values have been converted to their target datatype (in the converted dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a9d40",
   "metadata": {},
   "source": [
    "**Comparing the data types of columns in the 'acw_user_data.csv' file and those in the csv_with_new_type variable**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959cb2ef",
   "metadata": {},
   "source": [
    "**Note:**<br>The .create_list_of_lists() method helps to convert the structure of each row/record in a list from a  dictionary into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c9aa65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now converting the cleaner csv_dict_list (with appropriate data types) back into a list of lists\n",
    "# this is so I can zip both lists (csv_data and csv_with_new_type)\n",
    "# and iterate over them simultaneously\n",
    "csv_with_new_type = app.create_list_of_lists(csv_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06cb8407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a version of data in the csv file after cleaning up the datatype of some columns\n",
      "(e.g, 'Distance Commuted to Work (miles)', 'Yearly Pension (£)', 'Age (Years)', 'Credit Card Number', 'Credit Card CVV', etc).\n",
      "\n",
      "LINE 1\n",
      "Before Data Conversion:\n",
      "['70 Lydia isle', 'Lake Conor', 'S71 7XZ', '89', '0', 'N/A', '08/18', '11/27', '676373692463', '875', '3', 'Kieran', 'GB62PQKB71416034141571', 'Wilson', 'married or civil partner', '7257', 'True', '72838', 'Male', 'Hyundai', 'Bonneville', '2009', 'Pickup']\n",
      "\n",
      "After Data Conversion:\n",
      "['70 Lydia isle', 'Lake Conor', 'S71 7XZ', 89, 0.0, 'N/A', '08/18', '11/27', 676373692463, 875, 3, 'Kieran', 'GB62PQKB71416034141571', 'Wilson', 'married or civil partner', 7257, True, 72838, 'Male', 'Hyundai', 'Bonneville', 2009, 'Pickup']\n",
      "\n",
      "\n",
      "\n",
      "LINE 2\n",
      "Before Data Conversion:\n",
      "['00 Wheeler wells', 'Chapmanton', 'L2 7BT', '46', '13.72', 'Begum-Williams', '08/12', '11/26', '4529436854129855', '583', '1', 'Jonathan', 'GB37UMCO54540228728019', 'Thomas', 'married or civil partner', '0', 'False', '54016', 'Male', 'Nissan', 'ATS', '1996', 'Coupe']\n",
      "\n",
      "After Data Conversion:\n",
      "['00 Wheeler wells', 'Chapmanton', 'L2 7BT', 46, 13.72, 'Begum-Williams', '08/12', '11/26', 4529436854129855, 583, 1, 'Jonathan', 'GB37UMCO54540228728019', 'Thomas', 'married or civil partner', 0, False, 54016, 'Male', 'Nissan', 'ATS', 1996, 'Coupe']\n",
      "\n",
      "\n",
      "\n",
      "LINE 3\n",
      "Before Data Conversion:\n",
      "['Studio 33K Joel walk', 'Randallborough', 'ME3N 1GH', '22', '16.02', 'Hill-Wright', '11/19', '07/27', '4091726363083888495', '422', '1', 'Antony', 'GB40CVUE84011545859591', 'Jones', 'married or civil partner', '0', 'False', '68049', 'Male', 'GMC', 'Achieva', '2015', 'Convertible, Coupe']\n",
      "\n",
      "After Data Conversion:\n",
      "['Studio 33K Joel walk', 'Randallborough', 'ME3N 1GH', 22, 16.02, 'Hill-Wright', '11/19', '07/27', 4091726363083888495, 422, 1, 'Antony', 'GB40CVUE84011545859591', 'Jones', 'married or civil partner', 0, False, 68049, 'Male', 'GMC', 'Achieva', 2015, 'Convertible, Coupe']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"This is a version of data in the csv file after cleaning up the datatype of some columns\\n(e.g, 'Distance Commuted to Work (miles)', 'Yearly Pension (£)', 'Age (Years)', 'Credit Card Number', 'Credit Card CVV', etc).\\n\")\n",
    "ind, stop = -1, 3\n",
    "# print(f\"There are {len(new_data_type)} rows.\\n\")\n",
    "for old, new in zip(csv_data, csv_with_new_type):\n",
    "    ind += 1\n",
    "    if ind == 0:\n",
    "        continue  # skip the first row (containing column names)\n",
    "    print(f\"LINE {ind}\\nBefore Data Conversion:\\n{old}\\n\\nAfter Data Conversion:\\n{new}\\n\\n\\n\")\n",
    "    if ind == stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44416fd0-af6f-4d47-9cb6-8cbff2f68110",
   "metadata": {},
   "source": [
    "### <br>**b) Data Structure Conversion**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a3eb7-aa62-48f8-81bf-d1efbd1ac0be",
   "metadata": {},
   "source": [
    "Change the structure of each row from flat to nested Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a7443c3-5d9e-46b1-b2a3-f59bfbaf8846",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows, and 23 columns in the output list of dictionaries\n",
      "Vehicle sub-category created\n"
     ]
    }
   ],
   "source": [
    "# using csv_with_new_type as data source, \n",
    "# for each row, classify these columns in the original data set: ['Vehicle Make', 'Vehicle Model', 'Vehicle Year', 'Vehicle Type'],\n",
    "# into a smaller dictionary nested in the row,\n",
    "# and rename each of the above columns to ['make', 'model', 'year', 'type']\n",
    "# output is a list of nested dictionaries\n",
    "group, new_keys, search_for = 'Vehicle', ['make', 'model', 'year', 'type'], ['Vehicle Make', 'Vehicle Model', 'Vehicle Year', 'Vehicle Type']\n",
    "data_list = app.classify_from_csv_data(csv_with_new_type, group, search_for, new_keys)\n",
    "print(f'{group} sub-category created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a462bac2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows.\n",
      "First 2 rows are given below:\n",
      "\n",
      "{'Address Street': '70 Lydia isle', 'Address City': 'Lake Conor', 'Address Postcode': 'S71 7XZ', 'Age (Years)': 89, 'Distance Commuted to Work (miles)': 0.0, 'Employer Company': 'N/A', 'Credit Card Start Date': '08/18', 'Credit Card Expiry Date': '11/27', 'Credit Card Number': 676373692463, 'Credit Card CVV': 875, 'Dependants': 3, 'First Name': 'Kieran', 'Bank IBAN': 'GB62PQKB71416034141571', 'Last Name': 'Wilson', 'Marital Status': 'married or civil partner', 'Yearly Pension (£)': 7257, 'Retired': True, 'Yearly Salary (£)': 72838, 'Sex': 'Male', 'Vehicle': {'make': 'Hyundai', 'model': 'Bonneville', 'year': 2009, 'type': 'Pickup'}}\n",
      "\n",
      "{'Address Street': '00 Wheeler wells', 'Address City': 'Chapmanton', 'Address Postcode': 'L2 7BT', 'Age (Years)': 46, 'Distance Commuted to Work (miles)': 13.72, 'Employer Company': 'Begum-Williams', 'Credit Card Start Date': '08/12', 'Credit Card Expiry Date': '11/26', 'Credit Card Number': 4529436854129855, 'Credit Card CVV': 583, 'Dependants': 1, 'First Name': 'Jonathan', 'Bank IBAN': 'GB37UMCO54540228728019', 'Last Name': 'Thomas', 'Marital Status': 'married or civil partner', 'Yearly Pension (£)': 0, 'Retired': False, 'Yearly Salary (£)': 54016, 'Sex': 'Male', 'Vehicle': {'make': 'Nissan', 'model': 'ATS', 'year': 1996, 'type': 'Coupe'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 2\n",
    "print(f\"There are {len(data_list)} rows.\\nFirst {count} rows are given below:\\n\")\n",
    "app.display_lines_in_list(data_list, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27a333ff-c82a-4d5c-af39-14c77e91d4fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit Card sub-category created\n"
     ]
    }
   ],
   "source": [
    "# now using the output of the classify_from_csv_data method above, named data_list, as data source, \n",
    "# for each row, classify these columns in the original data set: ['Credit Card Start Date', 'Credit Card Expiry Date', 'Credit Card Number', 'Credit Card CVV', 'Bank IBAN'],\n",
    "# into a smaller dictionary nested in the row\n",
    "# and rename each of the above columns to ['start date', 'end date', 'number', 'cvv', 'IBAN']\n",
    "# output is a list of nested dictionaries\n",
    "group, new_keys, search_for = 'Credit Card', ['start_date', 'end_date', 'number', 'cvv', 'iban'], ['Credit Card Start Date', 'Credit Card Expiry Date', 'Credit Card Number', 'Credit Card CVV', 'Bank IBAN']\n",
    "data_list2 = app.classify_from_csv_data(data_list, group, search_for, new_keys)\n",
    "print(f'{group} sub-category created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cf1fccc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows.\n",
      "First 2 rows are given below:\n",
      "\n",
      "{'Address Street': '70 Lydia isle', 'Address City': 'Lake Conor', 'Address Postcode': 'S71 7XZ', 'Age (Years)': 89, 'Distance Commuted to Work (miles)': 0.0, 'Employer Company': 'N/A', 'Dependants': 3, 'First Name': 'Kieran', 'Last Name': 'Wilson', 'Marital Status': 'married or civil partner', 'Yearly Pension (£)': 7257, 'Retired': True, 'Yearly Salary (£)': 72838, 'Sex': 'Male', 'Vehicle': {'make': 'Hyundai', 'model': 'Bonneville', 'year': 2009, 'type': 'Pickup'}, 'Credit Card': {'start_date': '08/18', 'end_date': '11/27', 'number': 676373692463, 'cvv': 875, 'iban': 'GB62PQKB71416034141571'}}\n",
      "\n",
      "{'Address Street': '00 Wheeler wells', 'Address City': 'Chapmanton', 'Address Postcode': 'L2 7BT', 'Age (Years)': 46, 'Distance Commuted to Work (miles)': 13.72, 'Employer Company': 'Begum-Williams', 'Dependants': 1, 'First Name': 'Jonathan', 'Last Name': 'Thomas', 'Marital Status': 'married or civil partner', 'Yearly Pension (£)': 0, 'Retired': False, 'Yearly Salary (£)': 54016, 'Sex': 'Male', 'Vehicle': {'make': 'Nissan', 'model': 'ATS', 'year': 1996, 'type': 'Coupe'}, 'Credit Card': {'start_date': '08/12', 'end_date': '11/26', 'number': 4529436854129855, 'cvv': 583, 'iban': 'GB37UMCO54540228728019'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 2\n",
    "print(f\"There are {len(data_list)} rows.\\nFirst {count} rows are given below:\\n\")\n",
    "app.display_lines_in_list(data_list, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70907427-817f-4f51-b245-4a7b818988c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Address sub-category created\n"
     ]
    }
   ],
   "source": [
    "# now using the output of the classify_from_csv_data method above, named data_list2, as data source, \n",
    "# for each row, classify these columns in the original data set: ['Address Street', 'Address City', 'Address Postcode']\n",
    "# into a smaller dictionary nested in the row\n",
    "# and rename each of the above columns to ['street', 'city', 'postcode']\n",
    "# output is a list of nested dictionaries\n",
    "group, new_keys, search_for =  'Address', ['street', 'city', 'postcode'], ['Address Street', 'Address City', 'Address Postcode']\n",
    "final_data = app.classify_from_csv_data(data_list2, group, search_for, new_keys)\n",
    "print(f'{group} sub-category created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b8f796c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows.\n",
      "First 2 rows are given below:\n",
      "\n",
      "{'Age (Years)': 89, 'Distance Commuted to Work (miles)': 0.0, 'Employer Company': 'N/A', 'Dependants': 3, 'First Name': 'Kieran', 'Last Name': 'Wilson', 'Marital Status': 'married or civil partner', 'Yearly Pension (£)': 7257, 'Retired': True, 'Yearly Salary (£)': 72838, 'Sex': 'Male', 'Vehicle': {'make': 'Hyundai', 'model': 'Bonneville', 'year': 2009, 'type': 'Pickup'}, 'Credit Card': {'start_date': '08/18', 'end_date': '11/27', 'number': 676373692463, 'cvv': 875, 'iban': 'GB62PQKB71416034141571'}, 'Address': {'street': '70 Lydia isle', 'city': 'Lake Conor', 'postcode': 'S71 7XZ'}}\n",
      "\n",
      "{'Age (Years)': 46, 'Distance Commuted to Work (miles)': 13.72, 'Employer Company': 'Begum-Williams', 'Dependants': 1, 'First Name': 'Jonathan', 'Last Name': 'Thomas', 'Marital Status': 'married or civil partner', 'Yearly Pension (£)': 0, 'Retired': False, 'Yearly Salary (£)': 54016, 'Sex': 'Male', 'Vehicle': {'make': 'Nissan', 'model': 'ATS', 'year': 1996, 'type': 'Coupe'}, 'Credit Card': {'start_date': '08/12', 'end_date': '11/26', 'number': 4529436854129855, 'cvv': 583, 'iban': 'GB37UMCO54540228728019'}, 'Address': {'street': '00 Wheeler wells', 'city': 'Chapmanton', 'postcode': 'L2 7BT'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 2\n",
    "print(f\"There are {len(data_list)} rows.\\nFirst {count} rows are given below:\\n\")\n",
    "app.display_lines_in_list(data_list, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4bf955-e146-4477-8215-5d0a0bf6e344",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e454d-9089-4636-9701-ffee79a94b9b",
   "metadata": {},
   "source": [
    "### TASK 3: \n",
    "The client informs you that they have had difficulty with errors in the dependants \n",
    "column.<br> Some entries are empty (i.e. “ “ or “”), which may hinder your conversion \n",
    "from Task 2. <br>These should be changed into something meaningful when \n",
    "encountered. <br>\n",
    "**Print a list where all such error corrections take place.** <br>\n",
    "E.g. Problematic rows for dependants: [16, 58, 80, 98]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c4d2c4-9fea-4f14-98d9-58bc31a4b54e",
   "metadata": {},
   "source": [
    "#### SOLUTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14529737-7a4d-495b-b2b4-a4915025c02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows, and 23 columns in the output list of dictionaries\n"
     ]
    }
   ],
   "source": [
    "fpath = 'acw_user_data.csv'\n",
    "csv_data_dict = app.create_list_of_dicts(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cef38c-0a41-45cb-8a4f-bea6ad2c9be2",
   "metadata": {},
   "source": [
    "**Note:**<br>\n",
    "The .validate_empty_str() method displays the rows containing blank str values for the given column name (in this case, 'Dependants').<br>\n",
    "And returns the list of dictionaries with each blank str replaced with None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c60ab0a0-faab-4d21-bd3c-f505195c9459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problematic rows for Dependants: [21, 109, 179, 205, 270, 272, 274, 358, 460, 468, 579, 636, 679, 725, 822, 865, 917, 931, 983]\n",
      "19 rows\n"
     ]
    }
   ],
   "source": [
    "col_name = 'Dependants'\n",
    "valid_csv_data = app.validate_empty_str(csv_data_dict, col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eb0fff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 rows in the Dependants column are given below:\n",
      "\n",
      "\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "empty_str = [21, 109, 179, 205, 270, 272, 274, 358, 460, 468, 579, 636, 679, 725, 822, 865, 917, 931, 983]\n",
    "count, stop = 0, 3\n",
    "print(f'First {stop} rows in the {col_name} column are given below:\\n\\n')\n",
    "for ind in range(len(valid_csv_data)):\n",
    "    if ind not in empty_str:\n",
    "        continue\n",
    "    print(valid_csv_data[ind]['Dependants'])\n",
    "    if count == stop - 1:\n",
    "        break\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ae77d-4016-4fdf-8107-81b64a0a9544",
   "metadata": {},
   "source": [
    "___ \n",
    "\n",
    "### TASK 4: \n",
    "Write all records to a **processed.json** file in the JSON data format. <br>\n",
    "This should be a **list of dictionaries**, where each index of the list is a dictionary representing a singular \n",
    "person. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e32382-e9cb-4da0-917f-13428bd2c7b8",
   "metadata": {},
   "source": [
    "#### SOLUTION:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d52ef9",
   "metadata": {},
   "source": [
    "#### Highlights of steps taken to generate processed.json file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745b35d6",
   "metadata": {},
   "source": [
    "1. Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0cff0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n"
     ]
    }
   ],
   "source": [
    "fpath = 'acw_user_data.csv'\n",
    "\n",
    "converter = {'float': 'Distance Commuted to Work (miles)',\n",
    "             'bool': 'Retired',\n",
    "             'int': ('Age (Years)', 'Yearly Pension (£)', 'Yearly Salary (£)', \n",
    "                     'Credit Card Number', 'Credit Card CVV', 'Vehicle Year', 'Dependants')}\n",
    "\n",
    "csv_with_new_type = app.dtype_converter(fpath, converter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf65c2b",
   "metadata": {},
   "source": [
    "2. Create  in each row, 3 sub-dictionaries containing 'Vehicle', 'Credit Card' and 'Address' collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e4bdd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vehicle sub-category created\n",
      "Credit Card sub-category created\n",
      "Address sub-category created\n"
     ]
    }
   ],
   "source": [
    "group, new_keys, search_for = 'Vehicle', ['make', 'model', 'year', 'type'], ['Vehicle Make', 'Vehicle Model', 'Vehicle Year', 'Vehicle Type']\n",
    "data_list = app.classify_from_csv_data(csv_with_new_type, group, search_for, new_keys)\n",
    "print(f'{group} sub-category created')\n",
    "\n",
    "group, new_keys, search_for = 'Credit Card', ['start_date', 'end_date', 'number', 'cvv', 'iban'], ['Credit Card Start Date', 'Credit Card Expiry Date', 'Credit Card Number', 'Credit Card CVV', 'Bank IBAN']\n",
    "data_list2 = app.classify_from_csv_data(data_list, group, search_for, new_keys)\n",
    "print(f'{group} sub-category created')\n",
    "\n",
    "group, new_keys, search_for =  'Address', ['street', 'city', 'postcode'], ['Address Street', 'Address City', 'Address Postcode']\n",
    "final_data = app.classify_from_csv_data(data_list2, group, search_for, new_keys)\n",
    "print(f'{group} sub-category created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1748e72",
   "metadata": {},
   "source": [
    "#### Hence, as I aim to replicate the output in the screenshot of the example \"processed.json\" shown in page 5 of the \"ProgDSAI_ACW.pdf\" file,<br> \n",
    "I have gone further to ***rename each column*** to what I saw in the example 'processed.json'.<br> \n",
    "Then, I ***sorted each column*** to appear like in the example screenshot.<br>\n",
    "Such that in each row, 'first_name' column appears first, followed by 'second_name', then 'age', 'sex', etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494429c9",
   "metadata": {},
   "source": [
    "3. Rename some columns from their default name to simpler ones, using the 'old' and 'new' lists to guide the process.<br>\n",
    "E.g <br>\n",
    "'First Name' -> 'first_name',<br>\n",
    "'Last Name' -> 'second_name',<br>\n",
    "'Yearly Salary (£)' -> 'salary',<br>\n",
    "etc<br>\n",
    "***As shown in page 5 of the \"ProgDSAI_ACW.pdf\" file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c181b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "old = ['First Name', 'Last Name', 'Age (Years)', 'Sex', 'Retired', 'Marital Status',\n",
    "       'Dependants', 'Yearly Salary (£)', 'Yearly Pension (£)', 'Employer Company', 'Distance Commuted to Work (miles)']\n",
    "\n",
    "new = ['first_name', 'second_name', 'age', 'sex', 'retired', 'marital_status',\n",
    "       'dependants', 'salary', 'pension', 'company', 'commute_distance']\n",
    "\n",
    "renamed_data = app.rename_col_names_in_json(final_data, old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a4d4977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows.\n",
      "First 3 rows are given below:\n",
      "\n",
      "{'age': 89, 'commute_distance': 0.0, 'company': 'N/A', 'dependants': 3, 'first_name': 'Kieran', 'second_name': 'Wilson', 'marital_status': 'married or civil partner', 'pension': 7257, 'retired': True, 'salary': 72838, 'sex': 'Male', 'Vehicle': {'make': 'Hyundai', 'model': 'Bonneville', 'year': 2009, 'type': 'Pickup'}, 'Credit Card': {'start_date': '08/18', 'end_date': '11/27', 'number': 676373692463, 'cvv': 875, 'iban': 'GB62PQKB71416034141571'}, 'Address': {'street': '70 Lydia isle', 'city': 'Lake Conor', 'postcode': 'S71 7XZ'}}\n",
      "\n",
      "{'age': 46, 'commute_distance': 13.72, 'company': 'Begum-Williams', 'dependants': 1, 'first_name': 'Jonathan', 'second_name': 'Thomas', 'marital_status': 'married or civil partner', 'pension': 0, 'retired': False, 'salary': 54016, 'sex': 'Male', 'Vehicle': {'make': 'Nissan', 'model': 'ATS', 'year': 1996, 'type': 'Coupe'}, 'Credit Card': {'start_date': '08/12', 'end_date': '11/26', 'number': 4529436854129855, 'cvv': 583, 'iban': 'GB37UMCO54540228728019'}, 'Address': {'street': '00 Wheeler wells', 'city': 'Chapmanton', 'postcode': 'L2 7BT'}}\n",
      "\n",
      "{'age': 22, 'commute_distance': 16.02, 'company': 'Hill-Wright', 'dependants': 1, 'first_name': 'Antony', 'second_name': 'Jones', 'marital_status': 'married or civil partner', 'pension': 0, 'retired': False, 'salary': 68049, 'sex': 'Male', 'Vehicle': {'make': 'GMC', 'model': 'Achieva', 'year': 2015, 'type': 'Convertible, Coupe'}, 'Credit Card': {'start_date': '11/19', 'end_date': '07/27', 'number': 4091726363083888495, 'cvv': 422, 'iban': 'GB40CVUE84011545859591'}, 'Address': {'street': 'Studio 33K Joel walk', 'city': 'Randallborough', 'postcode': 'ME3N 1GH'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 3\n",
    "print(f\"There are {len(renamed_data)} rows.\\nFirst {count} rows are given below:\\n\")\n",
    "app.display_lines_in_list(renamed_data, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b0cb0",
   "metadata": {},
   "source": [
    "4. Sort each row to appear in the order given by 'order_of_appearance' list.<br>\n",
    "***To resemble what was seen on page 5 of the \"ProgDSAI_ACW.pdf\" file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "010147ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_of_appearance = ['first_name', 'second_name', 'age', 'sex', 'retired', 'marital_status', \n",
    "                       'dependants', 'salary', 'pension','company', 'commute_distance']\n",
    "\n",
    "sorted_data = app.sort_col_names(renamed_data, order_of_appearance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b087aefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows.\n",
      "First 3 rows are given below:\n",
      "\n",
      "{'first_name': 'Kieran', 'second_name': 'Wilson', 'age': 89, 'sex': 'Male', 'retired': True, 'marital_status': 'married or civil partner', 'dependants': 3, 'salary': 72838, 'pension': 7257, 'company': 'N/A', 'commute_distance': 0.0, 'Vehicle': {'make': 'Hyundai', 'model': 'Bonneville', 'year': 2009, 'type': 'Pickup'}, 'Credit Card': {'start_date': '08/18', 'end_date': '11/27', 'number': 676373692463, 'cvv': 875, 'iban': 'GB62PQKB71416034141571'}, 'Address': {'street': '70 Lydia isle', 'city': 'Lake Conor', 'postcode': 'S71 7XZ'}}\n",
      "\n",
      "{'first_name': 'Jonathan', 'second_name': 'Thomas', 'age': 46, 'sex': 'Male', 'retired': False, 'marital_status': 'married or civil partner', 'dependants': 1, 'salary': 54016, 'pension': 0, 'company': 'Begum-Williams', 'commute_distance': 13.72, 'Vehicle': {'make': 'Nissan', 'model': 'ATS', 'year': 1996, 'type': 'Coupe'}, 'Credit Card': {'start_date': '08/12', 'end_date': '11/26', 'number': 4529436854129855, 'cvv': 583, 'iban': 'GB37UMCO54540228728019'}, 'Address': {'street': '00 Wheeler wells', 'city': 'Chapmanton', 'postcode': 'L2 7BT'}}\n",
      "\n",
      "{'first_name': 'Antony', 'second_name': 'Jones', 'age': 22, 'sex': 'Male', 'retired': False, 'marital_status': 'married or civil partner', 'dependants': 1, 'salary': 68049, 'pension': 0, 'company': 'Hill-Wright', 'commute_distance': 16.02, 'Vehicle': {'make': 'GMC', 'model': 'Achieva', 'year': 2015, 'type': 'Convertible, Coupe'}, 'Credit Card': {'start_date': '11/19', 'end_date': '07/27', 'number': 4091726363083888495, 'cvv': 422, 'iban': 'GB40CVUE84011545859591'}, 'Address': {'street': 'Studio 33K Joel walk', 'city': 'Randallborough', 'postcode': 'ME3N 1GH'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 3\n",
    "print(f\"There are {len(sorted_data)} rows.\\nFirst {count} rows are given below:\\n\")\n",
    "app.display_lines_in_list(sorted_data, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c8e6a",
   "metadata": {},
   "source": [
    "5. Save to filesystem as \"processed.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37a31d49-4857-4207-8aa2-be2c8decdb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed.json\n"
     ]
    }
   ],
   "source": [
    "# Write data contained in the \"sorted_data\" list to a JSON file named \"processed.json\"\n",
    "filename = \"processed.json\"\n",
    "app.write_json_file_from_dictlist(filename, sorted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd91d9-177d-4730-8865-5f88e44912fb",
   "metadata": {},
   "source": [
    "___\n",
    "### TASK 5: \n",
    "You should **create two additional file outputs, retired.json and employed.json**,<br>\n",
    "these should contain all retired customers (as indicated by the retired field in the \n",
    "CSV), and all employed customers respectively (as indicated by the employer field in \n",
    "the CSV)<br> and be in the JSON data format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bead12-6a36-44e3-b1ef-57737f72e97b",
   "metadata": {},
   "source": [
    "#### SOLUTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2393941-f420-429d-9182-5df1f11f5858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .read_from_json() method, read in data from the processed.json file\n",
    "# and assign to filename variable for later use\n",
    "filename = \"processed.json\"\n",
    "cust_dict_list = app.read_from_json(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed634777-a65e-4801-a577-fe584873d383",
   "metadata": {},
   "source": [
    "**Note:** <br>Using the .filter_dictlist() method, I'm going to collect and store in a variable,<br>\n",
    "Only rows in the given column where the value matches the given condition(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e59a54d-282d-498e-a057-bf43ca56df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ('retired', 'company')  # column_names whose values are to be checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf4bf053-7cc6-405e-865f-e225e959b422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result now has 246 customers\n"
     ]
    }
   ],
   "source": [
    "# collect only those records where 'Retired' is 'True' and assign to variable\n",
    "retired_customers = app.filter_dictlist(cust_dict_list, col_names[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcdcfc77-0598-47d0-8f47-e6cf9914a392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result now has 754 customers\n"
     ]
    }
   ],
   "source": [
    "# collect only those records where 'Employer Company' != 'N/A' and assign to variable\n",
    "employed_customers = app.filter_dictlist(cust_dict_list, col_names[1], 'N/A', use_opposite_cond=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f3645b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retired_customers)+len(employed_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec9504-dbe6-4fd4-abfa-bf01b6916e1c",
   "metadata": {},
   "source": [
    "**Write data in list of dict format to a JSON file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "136e6385-91b5-417d-b629-bc6d93c434b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfile, efile = 'retired.json', 'employed.json'  # filenames for JSON files assigned to variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbedd6d-caf1-4821-b20d-783c108cd994",
   "metadata": {},
   "source": [
    "The .write_json_file_from_dictlist() method helps to create a JSON file with the given name in the 'OutputFiles' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc8e2148-a3fb-4ec7-b9ff-1fae41446a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retired.json\n"
     ]
    }
   ],
   "source": [
    "app.write_json_file_from_dictlist(rfile, retired_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21e0350c-ca39-45c8-b3f5-2768deb64cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employed.json\n"
     ]
    }
   ],
   "source": [
    "app.write_json_file_from_dictlist(efile, employed_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aad0982-5b19-4e8d-857e-82721202a8de",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9578d93-6f05-444e-9bc5-1bcb560e7558",
   "metadata": {},
   "source": [
    "### TASK 6: \n",
    "The client states that there may be some issues with credit card entries. <br>Any \n",
    "customers that have **more than 10 years** between their start and end date need \n",
    "writing to a separate file, called **remove_ccard.json**, in the JSON data format.<br> The\n",
    "client will manually deal with these later based on your output. They request that you \n",
    "**write a function to help perform this**, which accepts a single row from the CSV data, \n",
    "and outputs whether the row should be flagged. <br>This can then be used when \n",
    "determining whether to write the current person to the remove_ccard file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ebc0d-d6f3-4547-bd51-bb6de2f2219f",
   "metadata": {},
   "source": [
    "#### SOLUTION:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67544a3",
   "metadata": {},
   "source": [
    "Creating 'remove_ccard.json' from the 'acw_user_data.csv' file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab1d8cf",
   "metadata": {},
   "source": [
    "Step 1.<br>\n",
    "Create a list of dictionary directly from the 'acw_user_data.csv' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "669134df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows, and 23 columns in the output list of dictionaries\n"
     ]
    }
   ],
   "source": [
    "fpath = 'acw_user_data.csv'\n",
    "cust_dict_list = app.create_list_of_dicts(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae046d",
   "metadata": {},
   "source": [
    "Step 2.<br>\n",
    "Change some column values to more appropriate data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b7db9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n",
      "invalid literal for int() with base 10: ''\n",
      "Data  ignored\n"
     ]
    }
   ],
   "source": [
    "converter = {'float': 'Distance Commuted to Work (miles)',\n",
    "             'bool': 'Retired',\n",
    "             'int': ('Age (Years)', 'Yearly Pension (£)', 'Yearly Salary (£)', 'Credit Card Number', 'Credit Card CVV', 'Vehicle Year', 'Dependants')}\n",
    "csv_with_new_type = app.dtype_converter(cust_dict_list, converter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a60263",
   "metadata": {},
   "source": [
    "Step 3.<br>\n",
    "Create 3 sub-dictionaries ('Vehicle', 'Credit Card', 'Address') in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4362d94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vehicle sub-category created\n",
      "Credit Card sub-category created\n",
      "Address sub-category created\n"
     ]
    }
   ],
   "source": [
    "group, new_keys, search_for = 'Vehicle', ['make', 'model', 'year', 'type'], ['Vehicle Make', 'Vehicle Model', 'Vehicle Year', 'Vehicle Type']\n",
    "data_list = app.classify_from_csv_data(cust_dict_list, group, search_for, new_keys)\n",
    "print(f'{group} sub-category created')\n",
    "\n",
    "group, new_keys, search_for = 'Credit Card', ['start_date', 'end_date', 'number', 'cvv', 'iban'], ['Credit Card Start Date', 'Credit Card Expiry Date', 'Credit Card Number', 'Credit Card CVV', 'Bank IBAN']\n",
    "data_list2 = app.classify_from_csv_data(data_list, group, search_for, new_keys)\n",
    "print(f'{group} sub-category created')\n",
    "\n",
    "group, new_keys, search_for =  'Address', ['street', 'city', 'postcode'], ['Address Street', 'Address City', 'Address Postcode']\n",
    "final_data = app.classify_from_csv_data(data_list2, group, search_for, new_keys)\n",
    "print(f'{group} sub-category created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f6ec8",
   "metadata": {},
   "source": [
    "Step 4.<br>\n",
    "Rename column from values in the list, 'old' to corresponding values in the list, 'new'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8d44f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "old = ['First Name', 'Last Name', 'Age (Years)', 'Sex', 'Retired', 'Marital Status', 'Dependants', 'Yearly Salary (£)',\n",
    "      'Yearly Pension (£)', 'Employer Company', 'Distance Commuted to Work (miles)']\n",
    "new = ['first_name', 'second_name', 'age', 'sex', 'retired', 'marital_status', 'dependants', 'salary', 'pension',\n",
    "      'company', 'commute_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3ae91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_data = app.rename_col_names_in_json(final_data, old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4edb23dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows.\n",
      "First 3 rows are given below:\n",
      "\n",
      "{'age': 89, 'commute_distance': 0.0, 'company': 'N/A', 'dependants': 3, 'first_name': 'Kieran', 'second_name': 'Wilson', 'marital_status': 'married or civil partner', 'pension': 7257, 'retired': True, 'salary': 72838, 'sex': 'Male', 'Vehicle': {'make': 'Hyundai', 'model': 'Bonneville', 'year': 2009, 'type': 'Pickup'}, 'Credit Card': {'start_date': '08/18', 'end_date': '11/27', 'number': 676373692463, 'cvv': 875, 'iban': 'GB62PQKB71416034141571'}, 'Address': {'street': '70 Lydia isle', 'city': 'Lake Conor', 'postcode': 'S71 7XZ'}}\n",
      "\n",
      "{'age': 46, 'commute_distance': 13.72, 'company': 'Begum-Williams', 'dependants': 1, 'first_name': 'Jonathan', 'second_name': 'Thomas', 'marital_status': 'married or civil partner', 'pension': 0, 'retired': False, 'salary': 54016, 'sex': 'Male', 'Vehicle': {'make': 'Nissan', 'model': 'ATS', 'year': 1996, 'type': 'Coupe'}, 'Credit Card': {'start_date': '08/12', 'end_date': '11/26', 'number': 4529436854129855, 'cvv': 583, 'iban': 'GB37UMCO54540228728019'}, 'Address': {'street': '00 Wheeler wells', 'city': 'Chapmanton', 'postcode': 'L2 7BT'}}\n",
      "\n",
      "{'age': 22, 'commute_distance': 16.02, 'company': 'Hill-Wright', 'dependants': 1, 'first_name': 'Antony', 'second_name': 'Jones', 'marital_status': 'married or civil partner', 'pension': 0, 'retired': False, 'salary': 68049, 'sex': 'Male', 'Vehicle': {'make': 'GMC', 'model': 'Achieva', 'year': 2015, 'type': 'Convertible, Coupe'}, 'Credit Card': {'start_date': '11/19', 'end_date': '07/27', 'number': 4091726363083888495, 'cvv': 422, 'iban': 'GB40CVUE84011545859591'}, 'Address': {'street': 'Studio 33K Joel walk', 'city': 'Randallborough', 'postcode': 'ME3N 1GH'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 3\n",
    "print(f\"There are {len(renamed_data)} rows.\\nFirst {count} rows are given below:\\n\")\n",
    "app.display_lines_in_list(renamed_data, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f466e8e",
   "metadata": {},
   "source": [
    "Step 5.<br>\n",
    "Sort columns in each row according to how they appear in the 'order_of_appearance' list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9777f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_of_appearance = ['first_name', 'second_name', 'age', 'sex', 'retired', 'marital_status', \n",
    "                       'dependants', 'salary', 'pension','company', 'commute_distance']\n",
    "sorted_data = app.sort_col_names(renamed_data, order_of_appearance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98f37bd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows.\n",
      "First 3 rows are given below:\n",
      "\n",
      "{'first_name': 'Kieran', 'second_name': 'Wilson', 'age': 89, 'sex': 'Male', 'retired': True, 'marital_status': 'married or civil partner', 'dependants': 3, 'salary': 72838, 'pension': 7257, 'company': 'N/A', 'commute_distance': 0.0, 'Vehicle': {'make': 'Hyundai', 'model': 'Bonneville', 'year': 2009, 'type': 'Pickup'}, 'Credit Card': {'start_date': '08/18', 'end_date': '11/27', 'number': 676373692463, 'cvv': 875, 'iban': 'GB62PQKB71416034141571'}, 'Address': {'street': '70 Lydia isle', 'city': 'Lake Conor', 'postcode': 'S71 7XZ'}}\n",
      "\n",
      "{'first_name': 'Jonathan', 'second_name': 'Thomas', 'age': 46, 'sex': 'Male', 'retired': False, 'marital_status': 'married or civil partner', 'dependants': 1, 'salary': 54016, 'pension': 0, 'company': 'Begum-Williams', 'commute_distance': 13.72, 'Vehicle': {'make': 'Nissan', 'model': 'ATS', 'year': 1996, 'type': 'Coupe'}, 'Credit Card': {'start_date': '08/12', 'end_date': '11/26', 'number': 4529436854129855, 'cvv': 583, 'iban': 'GB37UMCO54540228728019'}, 'Address': {'street': '00 Wheeler wells', 'city': 'Chapmanton', 'postcode': 'L2 7BT'}}\n",
      "\n",
      "{'first_name': 'Antony', 'second_name': 'Jones', 'age': 22, 'sex': 'Male', 'retired': False, 'marital_status': 'married or civil partner', 'dependants': 1, 'salary': 68049, 'pension': 0, 'company': 'Hill-Wright', 'commute_distance': 16.02, 'Vehicle': {'make': 'GMC', 'model': 'Achieva', 'year': 2015, 'type': 'Convertible, Coupe'}, 'Credit Card': {'start_date': '11/19', 'end_date': '07/27', 'number': 4091726363083888495, 'cvv': 422, 'iban': 'GB40CVUE84011545859591'}, 'Address': {'street': 'Studio 33K Joel walk', 'city': 'Randallborough', 'postcode': 'ME3N 1GH'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 3\n",
    "print(f\"There are {len(sorted_data)} rows.\\nFirst {count} rows are given below:\\n\")\n",
    "app.display_lines_in_list(sorted_data, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08476e56",
   "metadata": {},
   "source": [
    "**Note:**<br>\n",
    "The .cc_duration_is_above_decade_detector() method helps to detect when the difference between two dates in 'mm/yy' format is greater than 10 years.<br>\n",
    "It takes in one dictionary (row) as its data source, and extracts the date values in 'start_date' and 'end_date' arguments respectively.<br>\n",
    "Then, it ***returns True*** if number of years between both dates is greater than 10 and ***False otherwise.***<br>\n",
    "Below, I shall only be checking for the **first ten dictionaries in \"cust_dict_list\"**, as I avoid having to go through the entire list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5887a06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# filter customer records with the help of the detector method\n",
    "# thus, in order for us to use the decade_detector method with a collection of dictionaries\n",
    "# there is need for us to iterate through the list of dicts while calling the method each time\n",
    "start_date_col_name, end_date_col_name = \"start_date\", \"end_date\"\n",
    "stop_at = 10\n",
    "ind = 0\n",
    "while ind < stop_at:  # iterating over each row of dictionary record\n",
    "    # print True for customers whose cc_duration is above 10 years and False if less than or equal to 10 yrs\n",
    "    print(app.cc_duration_is_above_decade_detector(sorted_data[ind], start_date_col_name, end_date_col_name))\n",
    "    ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a1368",
   "metadata": {},
   "source": [
    "Now, through a list comprehension, I shall collect and store, each dictionary output of the .cc_duration_is_above_decade_detector() method as rows.<br>\n",
    "And then, assign the resulting list to a variable called, ***\"customers_with_above_10yr_duration_cc\"***,<br>\n",
    "which contains the records of customers whose cc_duration is above ten years in the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "751a47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_col_name, end_date_col_name = \"start_date\", \"end_date\"\n",
    "customers_with_above_10yr_duration_cc = [sorted_data[ind] for ind in range(len(sorted_data)) if app.cc_duration_is_above_decade_detector(sorted_data[ind], start_date_col_name, end_date_col_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e7f5bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 193 rows.\n",
      "First 3 credit cards still having over ten year duration:\n",
      "\n",
      "\n",
      "{'first_name': 'Jonathan', 'second_name': 'Thomas', 'age': 46, 'sex': 'Male', 'retired': False, 'marital_status': 'married or civil partner', 'dependants': 1, 'salary': 54016, 'pension': 0, 'company': 'Begum-Williams', 'commute_distance': 13.72, 'Vehicle': {'make': 'Nissan', 'model': 'ATS', 'year': 1996, 'type': 'Coupe'}, 'Credit Card': {'start_date': '08/12', 'end_date': '11/26', 'number': 4529436854129855, 'cvv': 583, 'iban': 'GB37UMCO54540228728019'}, 'Address': {'street': '00 Wheeler wells', 'city': 'Chapmanton', 'postcode': 'L2 7BT'}}\n",
      "\n",
      "{'first_name': 'Julian', 'second_name': 'Potter', 'age': 43, 'sex': 'Male', 'retired': False, 'marital_status': 'single', 'dependants': 3, 'salary': 96645, 'pension': 0, 'company': 'Clark Group', 'commute_distance': 20.05, 'Vehicle': {'make': 'Lexus', 'model': 'S-Series', 'year': 1998, 'type': 'Van/Minivan'}, 'Credit Card': {'start_date': '07/12', 'end_date': '03/29', 'number': 36970652008212, 'cvv': 622, 'iban': 'GB92PWOB18937027850061'}, 'Address': {'street': '24 Heather locks', 'city': 'New Mathew', 'postcode': 'WD10 1LX'}}\n",
      "\n",
      "{'first_name': 'Clive', 'second_name': 'Evans', 'age': 67, 'sex': 'Male', 'retired': True, 'marital_status': 'single', 'dependants': 1, 'salary': 27964, 'pension': 28075, 'company': 'N/A', 'commute_distance': 0.0, 'Vehicle': {'make': 'Volvo', 'model': 'Express 3500 Passenger', 'year': 2013, 'type': 'SUV'}, 'Credit Card': {'start_date': '07/15', 'end_date': '04/27', 'number': 213199489183130, 'cvv': 389, 'iban': 'GB04VSAZ69128049616822'}, 'Address': {'street': '81 Goodwin dam', 'city': 'Griffinstad', 'postcode': 'G3 7ZX'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the first 3 rows in output data read from acw_user_data.csv file\n",
    "count = 3\n",
    "print(f\"There are {len(customers_with_above_10yr_duration_cc)} rows.\\nFirst {count} credit cards still having over ten year duration:\\n\\n\")\n",
    "app.display_lines_in_list(customers_with_above_10yr_duration_cc, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df429eaa",
   "metadata": {},
   "source": [
    "**Write data in list of dict format to a JSON file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5699eed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_ccard.json\n"
     ]
    }
   ],
   "source": [
    "newfile_name =  \"remove_ccard.json\"\n",
    "app.write_json_file_from_dictlist(newfile_name, customers_with_above_10yr_duration_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f72928-f4b8-494d-aea1-aad7991dec5e",
   "metadata": {},
   "source": [
    "___\n",
    "### TASK 7: \n",
    "You have been tasked with calculating some additional metrics which will be used for \n",
    "ranking customers. <br>You should **create a new data attribute for our customers called \n",
    "“Salary-Commute”**.<br> **Reading in from processed.json**: <br>\n",
    "a. Add, and calculate appropriately, this new attribute. <br>It should represent the \n",
    "**Salary** that a customer earns, **per mile** of their commute. <br>\n",
    "i. Note: <br>If a person travels 1 or fewer commute miles, then their salary\u0002commute would be just their salary.<br>\n",
    "b. **Sort these records by that new metric, in ascending order**. <br>\n",
    "c. Store the output file out as a JSON format, for a **commute.json file.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23d33e-dbd9-4708-a18a-aa7626c35213",
   "metadata": {},
   "source": [
    "#### SOLUTION:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d88d7-e1be-4480-93f4-876f7f83e997",
   "metadata": {},
   "source": [
    "The .column1_per_column2_from_json_file() method helps to compute the values of column1 as a fraction of column2.<br>\n",
    "That is, column1/column2. And returns the calculated fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed3af461-ebbc-4f10-b83b-f0e04704214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data from the \"processed.json\" file, internally using the .read_from_json() method to achieve this,\n",
    "# and assign to filename variable for later use\n",
    "filename = \"processed.json\"\n",
    "col_name1, col_name2, new_col_name = \"salary\", 'commute_distance', 'Salary-Commute'\n",
    "\n",
    "sal_comm = app.column1_per_column2_from_json_file(filename, col_name1, col_name2, new_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75a7674f-703d-49cf-81c6-f6bacd2c03d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows.\n",
      "First 3 rows are:\n",
      "\n",
      "{'first_name': 'Graeme', 'second_name': 'Jackson', 'age': 52, 'sex': 'Male', 'retired': False, 'marital_status': 'single', 'dependants': 2, 'salary': 17046, 'pension': 0, 'company': 'Smith, Birch and Burke', 'commute_distance': 5.52, 'Vehicle': {'make': 'Chevrolet', 'model': 'Rally Wagon 1500', 'year': 2011, 'type': 'SUV'}, 'Credit Card': {'start_date': '06/14', 'end_date': '04/29', 'number': 4713424668774153, 'cvv': 3053, 'iban': 'GB09ELJH35362236053720'}, 'Address': {'street': 'Studio 9 Reid lights', 'city': 'South Ryan', 'postcode': 'E27 9GY'}, 'Salary-Commute': 3088.04}\n",
      "\n",
      "{'first_name': 'Janet', 'second_name': 'Quinn', 'age': 30, 'sex': 'Female', 'retired': False, 'marital_status': 'married or civil partner', 'dependants': 1, 'salary': 17428, 'pension': 0, 'company': 'Wood-Davies', 'commute_distance': 5.64, 'Vehicle': {'make': 'Audi', 'model': 'Land Cruiser', 'year': 2000, 'type': 'SUV'}, 'Credit Card': {'start_date': '02/12', 'end_date': '02/25', 'number': 180012621284154, 'cvv': 392, 'iban': 'GB47KCJL06712308674300'}, 'Address': {'street': 'Studio 34r Wilkinson camp', 'city': 'Louisland', 'postcode': 'G7H 8FA'}, 'Salary-Commute': 3090.07}\n",
      "\n",
      "{'first_name': 'Peter', 'second_name': 'Burton', 'age': 63, 'sex': 'Male', 'retired': False, 'marital_status': 'married or civil partner', 'dependants': 2, 'salary': 17245, 'pension': 0, 'company': 'Graham, Watts and Hall', 'commute_distance': 5.58, 'Vehicle': {'make': 'Dodge', 'model': 'Sorento', 'year': 2016, 'type': 'Sedan, Coupe, Hatchback'}, 'Credit Card': {'start_date': '11/12', 'end_date': '07/22', 'number': 4058253691525, 'cvv': 458, 'iban': 'GB30EGRJ34862965423758'}, 'Address': {'street': 'Studio 38 James throughway', 'city': 'Leighton', 'postcode': 'M8E 7HJ'}, 'Salary-Commute': 3090.5}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 3\n",
    "print(f\"There are {len(sal_comm)} rows.\\nFirst {count} rows are:\\n\")\n",
    "app.display_lines_in_list(sal_comm, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf695f0-3383-4e28-844a-34d0067bccde",
   "metadata": {},
   "source": [
    "**Write data in list of dict format to a JSON file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2818514-78a8-4b82-97a1-433eae001fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commute.json\n"
     ]
    }
   ],
   "source": [
    "newfile_name = \"commute.json\"\n",
    "app.write_json_file_from_dictlist(newfile_name, sal_comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f49ab41",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f79d73",
   "metadata": {},
   "source": [
    "#### <center>DATA PROCESSING TASKS DONE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b0691",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
